{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f13db4e-4b6f-4f0d-8d84-04fe2c78341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModel,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b3c7dbf-0aec-4d0c-9df7-9d5c76e9b159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 819.58it/s]\n",
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.2 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../.cache/huggingface/hub/models--Unbabel--wmt22-cometkiwi-da/snapshots/b3a8aea5a5fc22db68a554b92b3d96eb6ea75cc9/checkpoints/model.ckpt`\n",
      "/fs/classhomes/fall2024/cmsc723/c7230018/miniconda3/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "# Import CometKiwi Model\n",
    "model_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\n",
    "model = load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6d7f918-8a3b-4da6-8030-d94da60c40b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "413b2271-dd36-4492-89c4-d6169959c4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2359296 || all params: 567496731 || trainable%: 0.4157373727673508\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d83e17-8c89-41b9-83e7-8e4e7703a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_qlora(module, name=\"<ROOT>\"):\n",
    "    '''\n",
    "    Replace Linear layers with LoRALinear layers, recursively.\n",
    "    '''\n",
    "    for attr_str, _ in module.named_children():\n",
    "        target_attr = getattr(module, attr_str)\n",
    "        if type(target_attr) == torch.nn.Linear and \"lora\" not in attr_str:\n",
    "            #print('replacing: ', name, attr_str)\n",
    "            print(target_attr)\n",
    "            \n",
    "    for name, immediate_child_module in module.named_children():\n",
    "        replace_qlora(immediate_child_module, name)\n",
    "\n",
    "replace_qlora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "396ffe07-e31e-4522-b9b7-4f8508d7d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "        r=16, \n",
    "        target_modules = ['query','key','value']\n",
    "    )\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "23e79bfc-572d-43b4-9359-45cb11d8488c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PeftModel(\n",
       "      (base_model): LoraModel(\n",
       "        (model): UnifiedMetric(\n",
       "          (encoder): XLMREncoder(\n",
       "            (model): XLMRobertaModel(\n",
       "              (embeddings): XLMRobertaEmbeddings(\n",
       "                (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "                (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "                (token_type_embeddings): Embedding(1, 1024)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (encoder): XLMRobertaEncoder(\n",
       "                (layer): ModuleList(\n",
       "                  (0-23): 24 x XLMRobertaLayer(\n",
       "                    (attention): XLMRobertaAttention(\n",
       "                      (self): XLMRobertaSdpaSelfAttention(\n",
       "                        (query): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=16, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (output): XLMRobertaSelfOutput(\n",
       "                        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                        (dropout): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                    )\n",
       "                    (intermediate): XLMRobertaIntermediate(\n",
       "                      (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                      (intermediate_act_fn): GELUActivation()\n",
       "                    )\n",
       "                    (output): XLMRobertaOutput(\n",
       "                      (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                      (dropout): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (layerwise_attention): LayerwiseAttention(\n",
       "            (scalar_parameters): ParameterList(\n",
       "                (0): Parameter containing: [torch.float32 of size 1]\n",
       "                (1): Parameter containing: [torch.float32 of size 1]\n",
       "                (2): Parameter containing: [torch.float32 of size 1]\n",
       "                (3): Parameter containing: [torch.float32 of size 1]\n",
       "                (4): Parameter containing: [torch.float32 of size 1]\n",
       "                (5): Parameter containing: [torch.float32 of size 1]\n",
       "                (6): Parameter containing: [torch.float32 of size 1]\n",
       "                (7): Parameter containing: [torch.float32 of size 1]\n",
       "                (8): Parameter containing: [torch.float32 of size 1]\n",
       "                (9): Parameter containing: [torch.float32 of size 1]\n",
       "                (10): Parameter containing: [torch.float32 of size 1]\n",
       "                (11): Parameter containing: [torch.float32 of size 1]\n",
       "                (12): Parameter containing: [torch.float32 of size 1]\n",
       "                (13): Parameter containing: [torch.float32 of size 1]\n",
       "                (14): Parameter containing: [torch.float32 of size 1]\n",
       "                (15): Parameter containing: [torch.float32 of size 1]\n",
       "                (16): Parameter containing: [torch.float32 of size 1]\n",
       "                (17): Parameter containing: [torch.float32 of size 1]\n",
       "                (18): Parameter containing: [torch.float32 of size 1]\n",
       "                (19): Parameter containing: [torch.float32 of size 1]\n",
       "                (20): Parameter containing: [torch.float32 of size 1]\n",
       "                (21): Parameter containing: [torch.float32 of size 1]\n",
       "                (22): Parameter containing: [torch.float32 of size 1]\n",
       "                (23): Parameter containing: [torch.float32 of size 1]\n",
       "                (24): Parameter containing: [torch.float32 of size 1]\n",
       "            )\n",
       "          )\n",
       "          (train_corr): RegressionMetrics()\n",
       "          (val_corr): ModuleList(\n",
       "            (0-2): 3 x RegressionMetrics()\n",
       "          )\n",
       "          (estimator): FeedForward(\n",
       "            (ff): Sequential(\n",
       "              (0): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "              (1): Tanh()\n",
       "              (2): Dropout(p=0.1, inplace=False)\n",
       "              (3): Linear(in_features=3072, out_features=1024, bias=True)\n",
       "              (4): Tanh()\n",
       "              (5): Dropout(p=0.1, inplace=False)\n",
       "              (6): Linear(in_features=1024, out_features=1, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (sentloss): MSELoss()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e809018-369a-4eb8-a23c-77fc3b4a899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs/classhomes/fall2024/cmsc723/c7230018/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/fs/classhomes/fall2024/cmsc723/c7230018/miniconda3/lib/python3.12/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /fs/classhomes/fall2024/cmsc723/c7230018/miniconda3/ ...\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/fs/classhomes/fall2024/cmsc723/c7230018/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "Predicting DataLoader 0: 100%|████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction({'scores': [0.833168625831604, 0.7671145796775818, 0.8827177882194519], 'system_score': 0.8276669979095459})\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"src\": \"The output signal provides constant sync so the display never glitches.\",\n",
    "        \"mt\": \"Das Ausgangssignal bietet eine konstante Synchronisation, so dass die Anzeige nie stört.\"\n",
    "    },\n",
    "    {\n",
    "        \"src\": \"Kroužek ilustrace je určen všem milovníkům umění ve věku od 10 do 15 let.\",\n",
    "        \"mt\": \"Кільце ілюстрації призначене для всіх любителів мистецтва у віці від 10 до 15 років.\"\n",
    "    },\n",
    "    {\n",
    "        \"src\": \"Mandela then became South Africa's first black president after his African National Congress party won the 1994 election.\",\n",
    "        \"mt\": \"その後、1994年の選挙でアフリカ国民会議派が勝利し、南アフリカ初の黒人大統領となった。\"\n",
    "    }\n",
    "]\n",
    "model_output = model.predict(data, batch_size=8, gpus=1)\n",
    "print (model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade893ab-ab0f-4d04-b876-19bd3866a18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
