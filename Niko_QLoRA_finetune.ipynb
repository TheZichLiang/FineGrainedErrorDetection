{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af436d1-5e69-4310-9b66-ded076615f0d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f13db4e-4b6f-4f0d-8d84-04fe2c78341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModel,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8028485-5fd3-40f1-b227-eb40dcdca24a",
   "metadata": {},
   "source": [
    "## Set up CometKiwi base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b3c7dbf-0aec-4d0c-9df7-9d5c76e9b159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc78f4075ee4eb7a77ee01fef44ea76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.2 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../.cache/huggingface/hub/models--Unbabel--wmt22-cometkiwi-da/snapshots/b3a8aea5a5fc22db68a554b92b3d96eb6ea75cc9/checkpoints/model.ckpt`\n",
      "Encoder model frozen.\n",
      "/home/neko/miniconda3/envs/723project/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "# Import CometKiwi Model\n",
    "model_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\n",
    "model = load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "413b2271-dd36-4492-89c4-d6169959c4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6296603 || all params: 565137435 || trainable%: 1.114171988978221\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20abb488-c01e-4798-afab-40a4639cf76b",
   "metadata": {},
   "source": [
    "## Finetuning Multilingual Uncased Bert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c0a16-4783-4201-a8f2-5ebd155395cf",
   "metadata": {},
   "source": [
    "### Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "104b0670-d0d1-4a78-ae5b-6bd0d351428d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40898 samples in training dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>translation</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>José Ortega y Gasset visited Husserl at Freibu...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>However, a disappointing ninth in China meant ...</td>\n",
       "      <td>Eine enttäuschende Neunte in China bedeutete j...</td>\n",
       "      <td>72.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his diary, Chase wrote that the release of ...</td>\n",
       "      <td>In seinem Tagebuch, Chase schrieb, dass die Ve...</td>\n",
       "      <td>48.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>Schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once North Pacific salmon die off after spawni...</td>\n",
       "      <td>Sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>89.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original  \\\n",
       "0  José Ortega y Gasset visited Husserl at Freibu...   \n",
       "1  However, a disappointing ninth in China meant ...   \n",
       "2  In his diary, Chase wrote that the release of ...   \n",
       "3  Heavy arquebuses mounted on wagons were called...   \n",
       "4  Once North Pacific salmon die off after spawni...   \n",
       "\n",
       "                                         translation        mean  \n",
       "0  1934 besuchte José Ortega y Gasset Husserl in ...  100.000000  \n",
       "1  Eine enttäuschende Neunte in China bedeutete j...   72.666667  \n",
       "2  In seinem Tagebuch, Chase schrieb, dass die Ve...   48.666667  \n",
       "3  Schwere Arquebuses auf Waggons montiert wurden...   67.000000  \n",
       "4  Sobald der nordpazifische Lachs nach dem Laich...   89.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Data preprocessing\n",
    "\n",
    "# training dataset\n",
    "# en-de\n",
    "training_dataset = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/en-de/en-de-train/train.ende.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "\n",
    "# en-zh\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/en-zh/en-zh-train/train.enzh.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "training_dataset = pd.concat((training_dataset, df))\n",
    "\n",
    "# et-en\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/et-en/et-en-train/train.eten.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "training_dataset = pd.concat((training_dataset, df))\n",
    "\n",
    "# ne-en\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/ne-en/ne-en-train/train.neen.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "training_dataset = pd.concat((training_dataset, df))\n",
    "\n",
    "# ro-en\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/ro-en/ro-en-train/train.roen.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "training_dataset = pd.concat((training_dataset, df))\n",
    "\n",
    "# ru-en\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/ru-en/ru-en-train/train.ruen.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "training_dataset = pd.concat((training_dataset, df))\n",
    "\n",
    "# si-en; this language can't be tokenized for some reason\n",
    "#df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/si-en/si-en-train/train.sien.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "#training_dataset = pd.concat((training_dataset, df))\n",
    "training_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "training_dataset = training_dataset[['original','translation','mean']]\n",
    "training_dataset['mean'] = pd.to_numeric(training_dataset['mean'], errors='coerce')\n",
    "training_dataset.dropna(inplace=True)\n",
    "training_dataset.to_csv('train.csv')\n",
    "print(f'{len(training_dataset)} samples in training dataset')\n",
    "training_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2e157-f144-4f2f-bab3-4244f0146b5a",
   "metadata": {},
   "source": [
    "### Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89d423e6-a089-4544-b8ef-70aef6ae7214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5842 samples in validation dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>translation</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Simultaneously, the Legion took part to the pa...</td>\n",
       "      <td>Gleichzeitig nahm die Legion an der Befriedung...</td>\n",
       "      <td>64.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He also begins an affair with Veronica Harring...</td>\n",
       "      <td>Er beginnt auch eine Affäre mit Veronica Harri...</td>\n",
       "      <td>83.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The urban morphology of these two local waters...</td>\n",
       "      <td>Die urbane Morphologie dieser beiden lokalen W...</td>\n",
       "      <td>92.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Important finds included a bronze axe in Wellw...</td>\n",
       "      <td>Wichtige Funde waren eine Bronzeaxt in Wellwoo...</td>\n",
       "      <td>95.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Previously, Englishmen had drunk mainly dark s...</td>\n",
       "      <td>Früher hatten Engländer vor allem dunkle Stout...</td>\n",
       "      <td>51.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original  \\\n",
       "0  Simultaneously, the Legion took part to the pa...   \n",
       "1  He also begins an affair with Veronica Harring...   \n",
       "2  The urban morphology of these two local waters...   \n",
       "3  Important finds included a bronze axe in Wellw...   \n",
       "4  Previously, Englishmen had drunk mainly dark s...   \n",
       "\n",
       "                                         translation       mean  \n",
       "0  Gleichzeitig nahm die Legion an der Befriedung...  64.166667  \n",
       "1  Er beginnt auch eine Affäre mit Veronica Harri...  83.500000  \n",
       "2  Die urbane Morphologie dieser beiden lokalen W...  92.833333  \n",
       "3  Wichtige Funde waren eine Bronzeaxt in Wellwoo...  95.166667  \n",
       "4  Früher hatten Engländer vor allem dunkle Stout...  51.833333  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# Evaluation Dataset\n",
    "# en-de\n",
    "validation_dataset = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/en-de/en-de-dev/dev.ende.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "\n",
    "# en-zh\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/en-zh/en-zh-dev/dev.enzh.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "validation_dataset = pd.concat((validation_dataset, df))\n",
    "\n",
    "# et-en\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/et-en/et-en-dev/dev.eten.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "validation_dataset = pd.concat((validation_dataset, df))\n",
    "\n",
    "# ne-en\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/ne-en/ne-en-dev/dev.neen.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "validation_dataset = pd.concat((validation_dataset, df))\n",
    "\n",
    "# ro-en\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/ro-en/ro-en-dev/dev.roen.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "validation_dataset = pd.concat((validation_dataset, df))\n",
    "\n",
    "# ru-en\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/ru-en/ru-en-dev/dev.ruen.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "validation_dataset = pd.concat((validation_dataset, df))\n",
    "\n",
    "# si-en\n",
    "#df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/si-en/si-en-dev/dev.sien.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "#validation_dataset = pd.concat((validation_dataset, df))\n",
    "validation_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "validation_dataset = validation_dataset[['original','translation','mean']]\n",
    "validation_dataset['mean'] = pd.to_numeric(validation_dataset['mean'], errors='coerce')\n",
    "validation_dataset.dropna(inplace=True)\n",
    "validation_dataset.to_csv('validation.csv')\n",
    "\n",
    "print(f'{len(validation_dataset)} samples in validation dataset')\n",
    "validation_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec931f-07d5-422a-9f73-43240de64ce0",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62076d96-49ce-4da9-ae34-e4e34de65704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5993 sammples in testing dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original</th>\n",
       "      <th>translation</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Sultan appoints judges, and can grant pard...</td>\n",
       "      <td>Der Sultan ernennt Richter und kann Begnadigun...</td>\n",
       "      <td>92.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Antisemitism in modern Ukraine Antisemitism an...</td>\n",
       "      <td>Antisemitismus in der modernen Ukraine Antisem...</td>\n",
       "      <td>89.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morales continued his feud with Buddy Rose, de...</td>\n",
       "      <td>Morales setzte seine Fehde mit Buddy Rose fort...</td>\n",
       "      <td>99.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>American Maury Tripp attended the Jamboree fro...</td>\n",
       "      <td>Der Amerikaner Maury Tripp besuchte das Jambor...</td>\n",
       "      <td>89.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>He bowled a series of bouncers at Viv Richards...</td>\n",
       "      <td>Er boomte eine Reihe von Bouncern bei Viv Rich...</td>\n",
       "      <td>69.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original  \\\n",
       "0  The Sultan appoints judges, and can grant pard...   \n",
       "1  Antisemitism in modern Ukraine Antisemitism an...   \n",
       "2  Morales continued his feud with Buddy Rose, de...   \n",
       "3  American Maury Tripp attended the Jamboree fro...   \n",
       "4  He bowled a series of bouncers at Viv Richards...   \n",
       "\n",
       "                                         translation       mean  \n",
       "0  Der Sultan ernennt Richter und kann Begnadigun...  92.666667  \n",
       "1  Antisemitismus in der modernen Ukraine Antisem...  89.833333  \n",
       "2  Morales setzte seine Fehde mit Buddy Rose fort...  99.166667  \n",
       "3  Der Amerikaner Maury Tripp besuchte das Jambor...  89.666667  \n",
       "4  Er boomte eine Reihe von Bouncern bei Viv Rich...  69.000000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Processing\n",
    "\n",
    "# Testing Dataset\n",
    "# en-de\n",
    "test_dataset = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/en-de/en-de/test20.ende.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "\n",
    "# en-zh\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/en-zh/en-zh/test20.enzh.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "test_dataset = pd.concat((test_dataset, df))\n",
    "\n",
    "# et-en\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/et-en/et-en/test20.eten.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "test_dataset = pd.concat((test_dataset, df))\n",
    "\n",
    "# ne-en\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/ne-en/ne-en/test20.neen.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "test_dataset = pd.concat((test_dataset, df))\n",
    "\n",
    "# ro-en\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/ro-en/ro-en/test20.roen.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "test_dataset = pd.concat((test_dataset, df))\n",
    "\n",
    "# ru-en\n",
    "df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/ru-en/ru-en/test20.ruen.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "test_dataset = pd.concat((test_dataset, df))\n",
    "\n",
    "# si-en\n",
    "#df = pd.read_csv('wmt-qe-2022-data/train-dev_data/task1_da/train/si-en/si-en/test20.sien.df.short.tsv', sep='\\t', on_bad_lines='skip')\n",
    "#test_dataset = pd.concat((test_dataset, df))\n",
    "test_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "test_dataset = test_dataset[['original','translation','mean']]\n",
    "test_dataset['mean'] = pd.to_numeric(test_dataset['mean'], errors='coerce')\n",
    "test_dataset.dropna(inplace=True)\n",
    "test_dataset.to_csv('test.csv')\n",
    "print(f'{len(test_dataset)} sammples in testing dataset')\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bcd0ab-b34f-4390-ae44-0ca4052f687a",
   "metadata": {},
   "source": [
    "### Convert to HuggingFace dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc7ea1f9-3141-45f0-93bc-bd5cee7a7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "train_dataset = pd.read_csv('train.csv')\n",
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "\n",
    "validation_dataset = pd.read_csv('validation.csv')\n",
    "validation_dataset = Dataset.from_pandas(validation_dataset)\n",
    "\n",
    "test_dataset = pd.read_csv('test.csv')\n",
    "test_dataset = Dataset.from_pandas(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6789854-eb18-40d3-acbb-2d2129e8a6cd",
   "metadata": {},
   "source": [
    "### Tokenize Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a8081e5-647d-4bf9-8161-dc226904a1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c5dd0bf90c44c48d46eae90280a9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40898 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508a1dfeba6247f6b7e354547dffa9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5842 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192994e9e94d4bee886e8bae3ce66e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5993 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  105879\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"original\"], examples[\"translation\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "print('Vocab size: ',tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b60cf0a0-6853-4e38-b858-3a544d5192da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelvant columns\n",
    "train_dataset = train_dataset.remove_columns(['Unnamed: 0','original','translation'])\n",
    "validation_dataset = validation_dataset.remove_columns(['Unnamed: 0','original','translation'])\n",
    "test_dataset = test_dataset.remove_columns(['Unnamed: 0','original','translation'])\n",
    "\n",
    "# Rename column names to correct format\n",
    "train_dataset = train_dataset.rename_column(\"mean\", \"labels\")\n",
    "validation_dataset = validation_dataset.rename_column(\"mean\", \"labels\")\n",
    "test_dataset = test_dataset.rename_column(\"mean\", \"labels\")\n",
    "\n",
    "# Format lists to torch tensors\n",
    "train_dataset.set_format(\"torch\")\n",
    "validation_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fd5609-1397-4df2-8fa2-d6461d04eb38",
   "metadata": {},
   "source": [
    "### Create Torch Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6bfea07-6e33-4599-948f-3d6bcf118120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(validation_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e338c2f-dbf5-44ac-831f-f096c91fed38",
   "metadata": {},
   "source": [
    "### Load Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3939acb-3afa-4596-9963-7d2749024022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 167357185 || all params: 167357185 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-multilingual-uncased\", num_labels=1)\n",
    "print_trainable_parameters(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "475b9572-2f68-4dfd-986e-d242df448063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(bert_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "033c0f60-0eae-493a-8736-f7e4046bf071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd90d0-2d1f-4b12-ab8a-9922318015fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "109ee6c1-e351-4d38-a025-09363608f801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f818e77ef7c640b1a39bf93c23c52aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15339 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "tensor([80.0000, 73.3333, 70.6667, 81.6667, 63.0000, 88.6667, 49.6667,  1.0000])\n",
      "\n",
      "\n",
      "input_ids\n",
      "tensor([[  101, 90379, 10876,  ...,     0,     0,     0],\n",
      "        [  101,   163,   119,  ...,     0,     0,     0],\n",
      "        [  101, 24497, 10592,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 86865, 33579,  ...,     0,     0,     0],\n",
      "        [  101, 50153, 11278,  ...,     0,     0,     0],\n",
      "        [  101, 14388,   117,  ...,     0,     0,     0]])\n",
      "\n",
      "\n",
      "token_type_ids\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "\n",
      "\n",
      "attention_mask\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "\n",
      "\n",
      "labels\n",
      "tensor([71.6667, 66.0000, 16.6667, 36.0000, 93.6667, 85.1667, 63.0000, 80.0000])\n",
      "\n",
      "\n",
      "input_ids\n",
      "tensor([[  101, 38297, 13141,  ...,     0,     0,     0],\n",
      "        [  101, 54931, 10707,  ...,     0,     0,     0],\n",
      "        [  101, 92692,   559,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 15185, 27959,  ...,     0,     0,     0],\n",
      "        [  101,   576, 13163,  ...,     0,     0,     0],\n",
      "        [  101, 10887, 12046,  ...,     0,     0,     0]])\n",
      "\n",
      "\n",
      "token_type_ids\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "\n",
      "\n",
      "attention_mask\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "\n",
      "\n",
      "labels\n",
      "tensor([27.0000, 77.6667, 95.0000, 94.3333, 58.6667, 29.6667, 83.3333, 14.0000])\n",
      "\n",
      "\n",
      "input_ids\n",
      "tensor([[  101, 95484, 10127,  ...,     0,     0,     0],\n",
      "        [  101, 41185, 12009,  ...,     0,     0,     0],\n",
      "        [  101, 10106,   117,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101, 23150, 59761,  ...,     0,     0,     0],\n",
      "        [  101, 10104, 16520,  ...,     0,     0,     0],\n",
      "        [  101, 11631, 23029,  ...,     0,     0,     0]])\n",
      "\n",
      "\n",
      "token_type_ids\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "\n",
      "\n",
      "attention_mask\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "bert_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc4813c-2aae-43cd-85f0-e1f930d32a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fbc31e-9463-4cc1-84cf-17240cd71fd9",
   "metadata": {},
   "source": [
    "## Set up data for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ade893ab-ab0f-4d04-b876-19bd3866a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mt_path = 'test_data_2023/task1_sentence_level/en-de/test.ende.final.mt'\n",
    "src_path = 'test_data_2023/task1_sentence_level/en-de/test.ende.final.src'\n",
    "with open(mt_path, 'r') as f:\n",
    "    mt = f.read().splitlines()\n",
    "    f.close()\n",
    "with open(mt_path, 'r') as f:\n",
    "    src = f.read().splitlines()\n",
    "    f.close\n",
    "\n",
    "data = []\n",
    "if len(mt) == len(src):\n",
    "    for (m,s) in list(zip(mt,src)):\n",
    "        d = {}\n",
    "        d['mt'] = m\n",
    "        d['src'] = s\n",
    "        data.append(d)\n",
    "else:\n",
    "    print('length of mt and src do not match')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e17d03c0-753d-4c97-832c-2bfa0898f786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|████████████████████████████████████████████████████████| 238/238 [00:11<00:00, 19.94it/s]\n"
     ]
    }
   ],
   "source": [
    "base_model_output = model.predict(data, batch_size=8, gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae9743e-cc83-4ca8-94af-9f7682aaf534",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a4bcd-638e-479c-9f5b-5a5ba86482bd",
   "metadata": {},
   "source": [
    "## Apply LoRa to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6d7f918-8a3b-4da6-8030-d94da60c40b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "396ffe07-e31e-4522-b9b7-4f8508d7d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "        r=16, \n",
    "        target_modules = ['query','key','value']\n",
    "    )\n",
    "\n",
    "lora_model = get_peft_model(lora_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b064a4e0-6df7-4f29-9ee6-fa680d2d86cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2359296 || all params: 567496731 || trainable%: 0.4157373727673508\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0116211-661d-4e7b-a231-11b5addf8eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|████████████████████████████████████████████████████████| 238/238 [00:12<00:00, 18.80it/s]\n"
     ]
    }
   ],
   "source": [
    "lora_model_output = model.predict(data, batch_size=8, gpus=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "323ca160-b20f-4502-bc80-4823c86d36b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_sum = 0\n",
    "for (b,l) in list(zip(base_model_output['scores'], lora_model_output['scores'])):\n",
    "    diff_sum += (b-l)\n",
    "diff_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7845ac-10a1-4421-acee-8cf1da8e132a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "723project",
   "language": "python",
   "name": "723project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
