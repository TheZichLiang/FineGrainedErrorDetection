{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af436d1-5e69-4310-9b66-ded076615f0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f13db4e-4b6f-4f0d-8d84-04fe2c78341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModel,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "def get_data(lang=None):\n",
    "    if lang.lower() == \"en-de\":\n",
    "        return (pd.read_csv('csv_data/train.csv').iloc[:6992], \n",
    "               pd.read_csv('csv_data/validation.csv').iloc[:1000], \n",
    "               pd.read_csv('csv_data/test.csv').iloc[:998])\n",
    "    elif lang.lower() == \"en-zh\":\n",
    "        return (pd.read_csv('csv_data/train.csv').iloc[6992:13964], \n",
    "               pd.read_csv('csv_data/validation.csv').iloc[1000:1996], \n",
    "               pd.read_csv('csv_data/test.csv').iloc[998:1995])\n",
    "    elif lang.lower() == \"ru-en\":\n",
    "        return (pd.read_csv('csv_data/train.csv').iloc[33898:], \n",
    "               pd.read_csv('csv_data/validation.csv').iloc[4842:], \n",
    "               pd.read_csv('csv_data/test.csv').iloc[4993:])\n",
    "    else:\n",
    "        return (pd.read_csv('csv_data/train.csv'), \n",
    "               pd.read_csv('csv_data/validation.csv'), \n",
    "               pd.read_csv('csv_data/test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8028485-5fd3-40f1-b227-eb40dcdca24a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Set up CometKiwi base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b3c7dbf-0aec-4d0c-9df7-9d5c76e9b159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003c5c01a0d7494594ee3b0e96008020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs/classhomes/fall2024/cmsc723/c7230021/miniconda3/lib/python3.12/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "# Import CometKiwi Model\n",
    "model_path = download_model(\"Unbabel/wmt22-cometkiwi-da\")\n",
    "model = load_from_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "413b2271-dd36-4492-89c4-d6169959c4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6296603 || all params: 565137435 || trainable%: 1.114171988978221\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20abb488-c01e-4798-afab-40a4639cf76b",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6fdd50-5eca-441d-a931-0cb471da66c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Import Dataset and Choose Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7ea1f9-3141-45f0-93bc-bd5cee7a7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "lang = \"en-de\"\n",
    "train_df, validation_df, test_df = get_data(lang=lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f105c-66d6-42d5-8e05-d3a2d353a600",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Normalization for Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96d1ca28-08d3-4b0e-91b6-a2d857d25834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_df['mean'] = scaler.fit_transform(train_df['mean'].values.reshape(-1, 1))\n",
    "validation_df['mean'] = scaler.transform(validation_df['mean'].values.reshape(-1, 1))\n",
    "test_df['mean'] = scaler.transform(test_df['mean'].values.reshape(-1, 1))\n",
    "\n",
    "# Convert the normalized DataFrames to Hugging Face Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6789854-eb18-40d3-acbb-2d2129e8a6cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tokenize Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a8081e5-647d-4bf9-8161-dc226904a1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70a2e44f2ff458586b56627c2b37bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6992 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2a5ab8bc7647f183930f8fa6fb2f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcbd072d1a7462098695601db1a3ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  105879\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-multilingual-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"original\"], \n",
    "        examples[\"translation\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "validation_dataset_tokenized = validation_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset_tokenized = test_dataset.map(tokenize_function, batched=True)\n",
    "print('Vocab size: ',tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b60cf0a0-6853-4e38-b858-3a544d5192da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelvant columns\n",
    "train_dataset_tokenized = train_dataset_tokenized.remove_columns(['Unnamed: 0','original','translation'])\n",
    "validation_dataset_tokenized = validation_dataset_tokenized.remove_columns(['Unnamed: 0','original','translation'])\n",
    "test_dataset_tokenized = test_dataset_tokenized.remove_columns(['Unnamed: 0','original','translation'])\n",
    "\n",
    "# Rename column names to correct format\n",
    "train_dataset_tokenized = train_dataset_tokenized.rename_column(\"mean\", \"labels\")\n",
    "validation_dataset_tokenized = validation_dataset_tokenized.rename_column(\"mean\", \"labels\")\n",
    "test_dataset_tokenized = test_dataset_tokenized.rename_column(\"mean\", \"labels\")\n",
    "\n",
    "# Format lists to torch tensors\n",
    "train_dataset_tokenized.set_format(\"torch\")\n",
    "validation_dataset_tokenized.set_format(\"torch\")\n",
    "test_dataset_tokenized.set_format(\"torch\")\n",
    "\n",
    "# small subset of entire training data for testing the training process\n",
    "\n",
    "train_dataset_tokenized = train_dataset_tokenized.shuffle(seed=42).select(range(1000))\n",
    "validation_dataset_tokenized = validation_dataset_tokenized.shuffle(seed=42).select(range(100))\n",
    "#test_dataset_tokenized = test_dataset_tokenized.shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fd5609-1397-4df2-8fa2-d6461d04eb38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create Torch Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6bfea07-6e33-4599-948f-3d6bcf118120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset_tokenized, shuffle=True, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(validation_dataset_tokenized, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset_tokenized, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e338c2f-dbf5-44ac-831f-f096c91fed38",
   "metadata": {},
   "source": [
    "## Load BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d9fa7-dc9b-4e89-8d50-74252d4c9420",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Configure Quantization and LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3939acb-3afa-4596-9963-7d2749024022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 81748993 || all params: 167357185 || trainable%: 48.84701723442588\n",
      "trainable params: 884736 || all params: 168241921 || trainable%: 0.5258713136067913\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# bnb_config sets up Quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "# lora_config sets up LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16, \n",
    "    target_modules = ['query','key','value'],\n",
    "    inference_mode=False\n",
    ")\n",
    "\n",
    "#bert_model = torch.load(\"finetuned_Bert.pth\", weights_only=False)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-multilingual-uncased\", num_labels=1, quantization_config=bnb_config)\n",
    "bert_model.gradient_checkpointing_enable()\n",
    "print_trainable_parameters(bert_model)\n",
    "\n",
    "# ADDS Q\n",
    "bert_model = prepare_model_for_kbit_training(bert_model)\n",
    "\n",
    "# ADDS LORA\n",
    "bert_model = get_peft_model(bert_model, lora_config)\n",
    "\n",
    "print_trainable_parameters(bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33e624-bc72-4df3-9b11-4f0fab4ae1e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Set up optimizer, learning rate scheduler, move model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "475b9572-2f68-4dfd-986e-d242df448063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(bert_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "033c0f60-0eae-493a-8736-f7e4046bf071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 20\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "debd90d0-2d1f-4b12-ab8a-9922318015fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "bert_model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53faf5-ddee-47da-9cbb-3ede54506076",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109ee6c1-e351-4d38-a025-09363608f801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d551e2c9334fdaa30cb7e4854d6c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs/classhomes/fall2024/cmsc723/c7230021/miniconda3/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/fs/classhomes/fall2024/cmsc723/c7230021/miniconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss = 0.2906, Validation Loss = 0.1533\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba3228d41cd47888c3d4ca7d76ae1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define Mean Squared Error Loss\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5  # Number of epochs to wait for improvement before stopping\n",
    "threshold = 0.0005  # Minimum improvement in validation loss\n",
    "best_valid_loss = float('inf')\n",
    "no_improvement_count = 0\n",
    "\n",
    "with open('log.csv', 'w', newline='') as f:\n",
    "   writer = csv.writer(f)\n",
    "   writer.writerow(['epoch', 'train_loss', 'valid_loss'])\n",
    "    \n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    bert_model.train()\n",
    "    progress_bar = tqdm(range(len(train_dataloader) + len(eval_dataloader)))\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = bert_model(**batch)\n",
    "        predictions = outputs.logits.squeeze()  # Ensure predictions are [batch_size]\n",
    "        loss = mse_loss(predictions, batch[\"labels\"])\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    bert_model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**batch)\n",
    "            predictions = outputs.logits.squeeze()\n",
    "            loss = mse_loss(predictions, batch[\"labels\"])\n",
    "            valid_losses.append(loss.item())\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "    # Compute average losses\n",
    "    train_loss = sum(train_losses) / len(train_losses)\n",
    "    valid_loss = sum(valid_losses) / len(valid_losses)\n",
    "    print(f\"Epoch {epoch+1}: Training Loss = {train_loss:.4f}, Validation Loss = {valid_loss:.4f}\")\n",
    "    \n",
    "    # Log losses\n",
    "    with open('log.csv', 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch + 1, train_loss, valid_loss])\n",
    "    \n",
    "    # Early stopping logic\n",
    "    if best_valid_loss - valid_loss > threshold:\n",
    "        best_valid_loss = valid_loss\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "        print(f\"No improvement for {no_improvement_count} epoch(s).\")\n",
    "    \n",
    "    if no_improvement_count >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d96006-77aa-43f8-b462-c9e859a3ebfe",
   "metadata": {},
   "source": [
    "### Bert Model Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0e098e-2d8d-4092-8d4b-aa07a0f43e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "bert_model.save_pretrained(f'Bert_QLoRA_{lang}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc7c401-23ce-4845-a3bf-d06e7ead93f5",
   "metadata": {},
   "source": [
    "### Plot train and validation loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67364ac0-6171-4600-933b-e95f6481ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "train_log = pd.read_csv('log.csv')\n",
    "train_log[['train_loss', 'valid_loss']].plot()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'MSE Loss Curve, BERT QLoRA {lang.upper()}')\n",
    "plt.show()\n",
    "plt.savefig(f'loss_qlora_{lang}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fbc31e-9463-4cc1-84cf-17240cd71fd9",
   "metadata": {},
   "source": [
    "## Run both models on `test.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd78911-5f4a-443e-90c1-a615d60b4139",
   "metadata": {},
   "source": [
    "### CometKiwi model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733cdc26-7215-4ca1-beb5-2fde2aef3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"src\"] = test_df[\"original\"]\n",
    "test_df[\"mt\"] = test_df[\"translation\"]\n",
    "test_input = list(test_df[[\"src\", \"mt\"]].iloc[i].to_dict() for i in range(test_df.shape[0]))\n",
    "result = model.predict(test_input, batch_size=batch_size, gpus=1);\n",
    "comet_kiwi_results = torch.tensor(result.scores, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184964d-d0a2-43dd-958c-72ca9f8f8808",
   "metadata": {},
   "source": [
    "### Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84882842-86e1-4e1f-8578-9e187822dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "bert_model.eval()\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        result.append(bert_model(**batch))\n",
    "bert_results = torch.cat([i.logits for i in result]).T.squeeze().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f062e-ab3d-4265-8488-84556ee373aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original data\n",
    "baseline_results = torch.Tensor(test_df[\"mean\"])\n",
    "# Calculate Mean Squared Error on normalized values \n",
    "mean_squared_test_error = ((comet_kiwi_results - baseline_results) ** 2).mean().item()\n",
    "print(f\"Mean Squared Test Error CometKiwi predictions(Normalized {lang.upp}): {mean_squared_test_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa205ac1-2031-4351-9059-b56e1eef3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_test_error = ((bert_results - baseline_results) ** 2).mean().item()\n",
    "print(f\"Mean Squared Test Error QLORABERT predictions(Normalized {lang.upper()}): {mean_squared_test_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be875041-fd07-4f6d-8115-4bba52e01e62",
   "metadata": {},
   "source": [
    "QLORABERT predictions were 0.0201.. so it's slightly worse than cometkiwi after training qlorabert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae62dab9-ba68-4c98-9cc2-87b31ce1a86f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
